{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre_Processing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZv1JJffgOwBisL+fMPuad",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helithiga/FYP/blob/main/Pre_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N_48BDigddg"
      },
      "source": [
        "**Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p75k4sJgCFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3e0730-6fcd-4409-d383-146d8033f8bd"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "  \n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXqRJN3-iTb_"
      },
      "source": [
        "**Input Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtCR83-_iZie"
      },
      "source": [
        "question = 'what is the incubation period of dengue?'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWHyUNgOh7ro"
      },
      "source": [
        "**To Lower Case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNOLYIG9iABR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82034f6a-f583-425c-ea19-878dd7f354ad"
      },
      "source": [
        "question = question.lower()\n",
        "print(question)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what are the causes of hypomagnesemia?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69UC9iT6i3K9"
      },
      "source": [
        "**Removing Punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyi6VxBMjLab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e12f5605-5fae-40c1-dd31-9dee9174e99a"
      },
      "source": [
        "question_p = \"\".join([char for char in question if char not in string.punctuation])\n",
        "print(question_p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what are the causes of hypomagnesemia\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIf9g3LjnguL"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rvo3weUnoxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d335bd2e-d431-49cb-f3ca-a9fb4e321ce0"
      },
      "source": [
        "question_words = word_tokenize(question_p)\n",
        "print(question_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['what', 'are', 'the', 'causes', 'of', 'hypomagnesemia']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JTd-ShjprFZ"
      },
      "source": [
        "**Removing Stop Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO5Zz2MirMbk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14933ee1-6e4c-42b8-ce9f-9d9afbd4da8f"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "question_words_wo_sw = [word for word in question_words if word not in stop_words]\n",
        "print(question_words_wo_sw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['causes', 'hypomagnesemia']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajKwC_DN-rMv"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pidy1evf-taA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "983cd3a3-8663-4a0b-8729-353f5387c9bf"
      },
      "source": [
        "'''lemmatizer = WordNetLemmatizer() \n",
        "question_words_lemma = [lemmatizer.lemmatize(word,pos='v') for word in question_words_wo_sw]\n",
        "print(question_words_lemma)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"lemmatizer = WordNetLemmatizer() \\nquestion_words_lemma = [lemmatizer.lemmatize(word,pos='v') for word in question_words_wo_sw]\\nprint(question_words_lemma)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXBtVpyKU8DY"
      },
      "source": [
        "**Detokenizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTuli4CATE7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd87e11c-eb21-4f69-9ceb-765044bb9eab"
      },
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "processed_question = TreebankWordDetokenizer().detokenize(question_words_wo_sw)\n",
        "print(processed_question)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "causes hypomagnesemia\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vhg0r_GC9wt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2688d425-774f-4177-d252-3912e5cf10f9"
      },
      "source": [
        "pip install stanza"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/8b/3a9e7a8d8cb14ad6afffc3983b7a7322a3a24d94ebc978a70746fcffc085/stanza-1.1.1-py3-none-any.whl (227kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 24.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20kB 24.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 40kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 61kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 71kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 81kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 215kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 225kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (50.3.2)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxZCIX0vDDXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7cefbd1-d4e9-429a-db15-cc89334ec54b"
      },
      "source": [
        "import stanza\n",
        "stanza.download('en', package='mimic', processors={'ner': 'i2b2'})\n",
        "nlp = stanza.Pipeline('en', package='mimic', processors={'ner': 'i2b2'})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 23.6MB/s]                    \n",
            "2020-11-18 08:19:17 INFO: Downloading these customized packages for language: en (English)...\n",
            "=============================\n",
            "| Processor       | Package |\n",
            "-----------------------------\n",
            "| tokenize        | mimic   |\n",
            "| pos             | mimic   |\n",
            "| lemma           | mimic   |\n",
            "| depparse        | mimic   |\n",
            "| ner             | i2b2    |\n",
            "| pretrain        | mimic   |\n",
            "| forward_charlm  | mimic   |\n",
            "| backward_charlm | mimic   |\n",
            "=============================\n",
            "\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/tokenize/mimic.pt: 100%|██████████| 631k/631k [00:00<00:00, 2.15MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/pos/mimic.pt: 100%|██████████| 20.8M/20.8M [00:04<00:00, 4.31MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/lemma/mimic.pt: 100%|██████████| 4.19M/4.19M [00:00<00:00, 6.12MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/depparse/mimic.pt: 100%|██████████| 109M/109M [00:04<00:00, 24.1MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/ner/i2b2.pt: 100%|██████████| 123M/123M [00:07<00:00, 17.1MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/pretrain/mimic.pt: 100%|██████████| 123M/123M [00:20<00:00, 5.86MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/forward_charlm/mimic.pt: 100%|██████████| 18.9M/18.9M [00:01<00:00, 11.5MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/backward_charlm/mimic.pt: 100%|██████████| 18.9M/18.9M [00:02<00:00, 8.20MB/s]\n",
            "2020-11-18 08:20:11 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2020-11-18 08:20:11 INFO: Loading these models for language: en (English):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | mimic   |\n",
            "| pos       | mimic   |\n",
            "| lemma     | mimic   |\n",
            "| depparse  | mimic   |\n",
            "| ner       | i2b2    |\n",
            "=======================\n",
            "\n",
            "2020-11-18 08:20:11 INFO: Use device: cpu\n",
            "2020-11-18 08:20:11 INFO: Loading: tokenize\n",
            "2020-11-18 08:20:11 INFO: Loading: pos\n",
            "2020-11-18 08:20:12 INFO: Loading: lemma\n",
            "2020-11-18 08:20:12 INFO: Loading: depparse\n",
            "2020-11-18 08:20:13 INFO: Loading: ner\n",
            "2020-11-18 08:20:13 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c6dcYDgQzWV"
      },
      "source": [
        "**NER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfNoLRNgJ3hd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b83434d-319a-4592-db88-0e8c1028d252"
      },
      "source": [
        "question_words_ner = nlp('Will a person with Type 2 diabetes under control end up with the need for insulin?')\n",
        "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in question_words_ner.ents], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "entity: Type 2 diabetes\ttype: PROBLEM\n",
            "entity: insulin\ttype: TREATMENT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWzKuNCGU4XN"
      },
      "source": [
        "**POS Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjXvchsMOew0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8203499b-8c0b-4bcb-c37c-3d7c91592b9f"
      },
      "source": [
        "question_words_pos = nlp('how much')\n",
        "print(*[f'entity: {ent.text}\\txpos: {ent.xpos}\\tlemma: {ent.lemma}' \n",
        "        for sent in question_words_pos.sentences for ent in sent.words], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "entity: how\txpos: WRB\tlemma: how\n",
            "entity: much\txpos: JJ\tlemma: much\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zd8VGOwu5wM"
      },
      "source": [
        "**All in One**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TR38K1Ru8LI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "6cef6a14-0e16-4bf0-934c-af4fc47b8ae2"
      },
      "source": [
        "'''\n",
        "pip install stanza\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nltk.download('punkt')\n",
        "\n",
        "question = 'It is a truth universally acknowledged, that {a} single man in [possession] of a good fortune, must be in want! of a wife.'\n",
        "\n",
        "question = question.lower()\n",
        "print(question)\n",
        "\n",
        "question_p = \"\".join([char for char in question if char not in string.punctuation])\n",
        "print(question_p)\n",
        "\n",
        "question_words = word_tokenize(question_p)\n",
        "print(question_words)\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "question_words_wo_sw = [word for word in question_words if word not in stop_words]\n",
        "print(question_words_wo_sw)\n",
        "\n",
        "stemmer= PorterStemmer()\n",
        "for word in question_words_wo_sw:\n",
        "    print(stemmer.stem(word))\n",
        "\n",
        "#text = 'Will a person with Type 2 diabetes under control end up with the need for insulin?'\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "question_words_POS = nlp(question_p)\n",
        "displacy.render(question_words_POS, style='dep', jupyter=True,options={'distance':100})'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\npip install stanza\\n\\nimport nltk\\nimport string\\nimport re\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.stem import PorterStemmer\\nimport spacy\\nfrom spacy import displacy\\nnltk.download(\\'punkt\\')\\n\\nquestion = \\'It is a truth universally acknowledged, that {a} single man in [possession] of a good fortune, must be in want! of a wife.\\'\\n\\nquestion = question.lower()\\nprint(question)\\n\\nquestion_p = \"\".join([char for char in question if char not in string.punctuation])\\nprint(question_p)\\n\\nquestion_words = word_tokenize(question_p)\\nprint(question_words)\\n\\nstop_words = stopwords.words(\\'english\\')\\nquestion_words_wo_sw = [word for word in question_words if word not in stop_words]\\nprint(question_words_wo_sw)\\n\\nstemmer= PorterStemmer()\\nfor word in question_words_wo_sw:\\n    print(stemmer.stem(word))\\n\\n#text = \\'Will a person with Type 2 diabetes under control end up with the need for insulin?\\'\\nnlp = spacy.load(\"en_core_web_sm\")\\nquestion_words_POS = nlp(question_p)\\ndisplacy.render(question_words_POS, style=\\'dep\\', jupyter=True,options={\\'distance\\':100})'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}